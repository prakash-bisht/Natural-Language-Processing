{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import MWETokenizer \n",
    "from nltk.util import ngrams \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "text = \"hope everything is fine \"\n",
    "#print(word_tokenize(text))\n",
    "s_text = \"Hi Mr. Smith! I'm going to buy some vegetables (tomatoes and cucumber from the store. Should I pick up some black-eyed peas as well?\"\n",
    "#print(sent_tokenize(s_text))\n",
    "#print(list(ngrams(word_tokenize(text),2)))\n",
    "whitespace_exp = RegexpTokenizer('\\s+', gaps = True)\n",
    "#print(whitespace_exp.tokenize(text))\n",
    "exp = RegexpTokenizer(\"[A-Z][\\w]+\")\n",
    "#print(exp.tokenize(s_text))\n",
    "c_text = re.sub('[%s]' % re.escape(string.punctuation),'', s_text)\n",
    "#print(c_text)\n",
    "data = c_text.lower()\n",
    "#print(l)\n",
    "data = re.sub('\\w*\\d\\w*', ' ',data )\n",
    "#print(data)\n",
    "s_me = lambda x : x * x\n",
    "#print(s_me(5))\n",
    "#number = [2,1,4,5]\n",
    "#print(list(map(s_me, number)))\n",
    "text1 = \"i got you for 5times\"\n",
    "text2 = \"i will have 6 apples\"\n",
    "text3 = \"i do not want to know the number5\"\n",
    "texts = [text1, text2, text3]\n",
    "text = lambda x : re.sub('\\w*\\d\\w*', '', x)\n",
    "#print(list(map(text,texts)))\n",
    "#set(stopwords.words('english'))\n",
    "m_text = [\"Hi Mr. Smith! I'm going to buy some vegetables (tomatoes and cucumber from the store. Should I pick up some black-eyed peas as well?\"]\n",
    "cv = CountVectorizer(stop_words = \"english\")\n",
    "x = cv.fit_transform(m_text)\n",
    "x = pd.DataFrame(x.toarray(), columns = cv.get_feature_names())\n",
    "stemmer = LancasterStemmer()\n",
    "'''print(\"drive : {}\".format(stemmer.stem(\"drive\")))\n",
    "print(\"drive : {}\".format(stemmer.stem(\"driving\")))\n",
    "print(\"drive : {}\".format(stemmer.stem(\"drives\"))) '''\n",
    "t = \"James Smith lives in the United States.\"\n",
    "p = ne_chunk(pos_tag(word_tokenize(t)))\n",
    "#p.draw()\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
